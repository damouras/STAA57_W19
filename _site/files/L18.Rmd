<!-- 
output: html_notebook 
-->


```{r setup, include=FALSE}
rm(list = ls())
library(tidyverse)
library(pROC)
wdbc = read_csv("data/wdbc.csv")
wdbc %>% 
  ggplot(aes(x = fract.dim.m, y = radius.m, col = diagnosis)) + 
  geom_point() + scale_y_continuous(limit = c(0,30)) + 
  coord_fixed(ratio = 1/100) + 
  ggsave("img/scatter.png", height = 5, width = 7)
set.seed(123)
```

## Lecture Goals 

- Work with multiple features  
- Understand linear & nonlinear classification
- Perform multivariate classification in R with
    + *Logistic Regression*
    + *Classification Trees*


- Readings: 
    + [ISLR](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) ch 4.3-4 (optional)

## Multiple Features

- Information can be reflected in multiple variables

- **Feature Engineering**: which features should be *generated* from available information?
    + E.g. in WDBC features represent cell morphology 

- **Feature Selection**: which features from *fixed* collection should one use? 
    + Not always optimal to use *all* features

## WDBC Data 

- Thresholding *individual* variable gives OK classifier
- Can it improve by combining *multiple* variables?

```{r, echo=FALSE, fig.width=6, fig.height=4.5}
wdbc %>% 
  ggplot(aes(x = fract.dim.m, y = radius.m, col = diagnosis)) +
  geom_point() + geom_vline( xintercept = .12, lty = 3) + 
  geom_hline( yintercept = 14, lty = 3) + 
  geom_rug()

```


## Linear Classifiers

- Linear (binary) classifier uses *linear decision boundary* (i.e. threshold) to classificy observations
    + Boundary is line in 2D, plane in 3D feature space
    
- Class determined by distance (+/-) from boundary 
    + Distance expressed as *linear function* of features
 
- Classification function reduces to 
   $$f(X_1,\ldots, X_p) = \mathrm{sign}( \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p) $$
     + Parameter $\beta_0$ controls distance from *boundary*  

## Linear Classifiers

![](img/linear_classifier.PNG)


## Linear Classifiers 

- Several ways to find linear decision boundary
    + Logistic Regression
    + Linear Discriminant Analysis (LDA)
    + Support Vector Machines (SVM)
    
- All methods produce *normal vector* of linear coefficients ($\vec{\beta}$)  

- We will focus on logistic regression

## Logistic Regression

- Fit logistic regression with `glm()`

```{r, collapse=TRUE}
glm_out = glm( diagnosis ~ fract.dim.m + radius.m, family = "binomial", 
  data = wdbc %>% mutate( diagnosis = factor(diagnosis)) ) 
broom::tidy(glm_out)
```
- Decision boundary $-18.59 + 52.06 X_1 + 0.81 X_2 = 0$



## Logistic Regression


```{r, echo=FALSE}
b0.lgst = glm_out$coefficients[1]
b.lgst = glm_out$coefficients[2:3]

wdbc %>% 
  ggplot(aes(x = fract.dim.m, y = radius.m, col = diagnosis)) +
  geom_point() + 
  geom_abline( intercept =  -b0.lgst/b.lgst[2], slope = -b.lgst[1]/b.lgst[2], 
               linetype = 2, lwd = 1.2) +
  geom_text(x = 0.015, y = 18, label = "logistic \n decision \n boundary", col = "black")  

```


## Logistic Regression

- `predict()` returns *distance* from *decision boundary*
    + i.e. projection + threshold
```{r, collapse = TRUE}
( glm_pred = predict(glm_out) ) %>% sample(5)

wdbc %>% modelr::add_predictions(glm_out, var = "distance") %>% 
  mutate( predicted = ifelse(distance < 0, "B", "M")  ) %>% 
  xtabs(~ predicted + diagnosis, data = .) %>% prop.table()
```



## Logistic Regression

```{r, echo = FALSE, fig.height=5, fig.width=7}
wdbc %>% modelr::add_predictions(glm_out, var = "distance") %>% 
  ggplot(aes(x = distance, fill = diagnosis)) + 
  geom_histogram(bins = 30, position = "dodge") + 
  geom_vline( xintercept = 0, lty = 2, lwd = 1.3)

```


## Single vs Multiple Features 

- Compare ROC curves 
```{r, echo=FALSE, fig.height=5, fig.width=7}
ROC.thres = roc(diagnosis ~ fract.dim.m,  data = wdbc) 
ROC.lda =  roc(diagnosis ~ distance,  data = wdbc %>% 
                 modelr::add_predictions(glm_out, var = "distance") )
ggroc(list( ROC.thres, ROC.lda), lwd =1) +
  scale_color_discrete( labels = c("1 feature (threshold fract.dim)", "2 features (logistic: fract.dim + radius)") )
```



## Nonlinear Classifiers

- Classifier with *nonlinear* decision boundaries
    
![](img/non_linear_classifier.PNG)

## Nearest Neighbor (NN) 

- Basics nonlinear classifier: classify each point like its *nearest neighbor*
 
```{r, echo = FALSE}
#install.packages("ggvoronoi")
library(ggvoronoi)

wdbc %>% mutate( fract.dim.m = scale(fract.dim.m), radius.m = scale(radius.m) ) %>% 
  ggplot( aes(x = fract.dim.m, y = radius.m) ) +
  geom_point(aes(col = diagnosis)) + 
  stat_voronoi(geom="path", lty=2, col = 'grey') +
  geom_voronoi(aes(fill = diagnosis), alpha = .2) + 
  guides(fill=FALSE) 
```

## Classification Tree

- Threshold different variables in *nested/hierarchical* manner 

```{r, echo = FALSE}
regions = tibble( x = c(0,0,17,17, 0,0,40,40,17,17 ), 
                      y = c(-.05,.14,.14,-.05, .14,.32,.32,-.05,-.05,.14 ), 
                      group = factor( c( rep("B",4),  rep("M",6) ) ) )

ggplot(wdbc) +
  geom_point( aes(x = concavity.w, y = fract.dim.m, col = diagnosis )) +
  geom_vline( xintercept = 17, lty = 2, lwd = 2 ) + 
  coord_cartesian( xlim = c(7,37), ylim = c( -.01, .3) ) +
  geom_segment( x= 0, xend = 17, y = .14, yend=.14, lty = 2, lwd = 2, col = "black") +
  geom_polygon(data = regions, aes(x = x, y = y, fill = group), alpha =.2)  + 
  guides(fill=FALSE) 

```


## Classification Tree

- Fit tree using library `rpart` (recursive partitioning)

```{r, collapse=TRUE}
library(rpart)
rpart_out = rpart( diagnosis ~ . - id, data = wdbc, 
    method = "class", control = rpart.control(minsplit=50) )
rpart_out
```

## Classification Tree

- Plot tree using library `rpart.plot`

```{r, fig.width=5.5, fig.height=4}
library(rpart.plot); rpart.plot(rpart_out)
```

## Classification Tree Predictions

- `predict()` gives class *probabilities* 
    + Use `type = "class"` to get classes

```{r, collapse=TRUE}
predict(rpart_out) %>% head(2)

wdbc %>% modelr::add_predictions( rpart_out, type = "class" ) %>% 
  xtabs( ~ pred + diagnosis, data = .) %>% prop.table()
```


          






<!-- 



## LDA

```{r, collapse = TRUE}
# Fit LDA model using MASS::lda() function
lda.model = MASS::lda(diagnosis ~ fract.dim.m + radius.m, data = wdbc)
lda.pred = predict(lda.model) # extract model predictions & more
names(lda.pred)

wdbc %>% mutate( predicted = lda.pred$class ) %>% 
  select(predicted, diagnosis) %>% table()
```


## LDA vs Logistic 


```{r, echo=FALSE}
b.lda = lda.model$scaling
b0.lda = mean( lda.model$means %*% b.lda) 

b0.lgst = glm_out$coefficients[1]
b.lgst = glm_out$coefficients[2:3]

wdbc %>% 
  ggplot(aes(x = fract.dim.m, y = radius.m, col = diagnosis)) +
  geom_point() + 
  geom_abline( intercept =  b0.lda/b.lda[2], slope = -b.lda[1]/b.lda[2], 
               linetype = 3, lwd = 1.2) + 
  geom_text(x = .035, y = 25, label = "LDA", col = "black")  +
  geom_abline( intercept =  -b0.lgst/b.lgst[2], slope = -b.lgst[1]/b.lgst[2], 
               linetype = 2, lwd = 1.2) +
  geom_text(x = .025, y = 20, label = "Logistic", col = "black")  

```


## Nearest Neighbor (NN) 

- Memorize data and classify each point according to its nearest neighbor
 
```{r, echo = FALSE}
#install.packages("ggvoronoi")
library(ggvoronoi)

wdbc %>% mutate( fract.dim.m = scale(fract.dim.m), radius.m = scale(radius.m) ) %>% 
  ggplot( aes(x = fract.dim.m, y = radius.m) ) +
  geom_point(aes(col = diagnosis)) + 
  stat_voronoi(geom="path", lty=2, col = 'grey') +
  geom_voronoi(aes(fill = diagnosis), alpha = .2) + 
  guides(fill=FALSE) 
```
-->

